{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52db1333-550f-4d4e-8697-2c1e0350cd28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57f64373-af0f-4f0d-a616-873120339af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Demo - Using Feature Store for Feature Engineering \n",
    "\n",
    "In this demo, we will guide you to explore the use of Feature Stores to enhance feature engineering workflow and understand their crucial role in development of machine learning models. First we will create feature store tables for effective implementation in feature engineering processes and then discuss how to update features. Also, we will cover how to convert existing table to feature tables in Unity Catalog.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to;*\n",
    "\n",
    "1. Create a Feature Store table from a PySpark DataFrame for training/modeling data and holdout data.\n",
    "1. Identify the requirements for a Delta table in Unity Catalog to be automatically configured as a feature table.\n",
    "1. Alter an existing Delta table in Unity Catalog so that it can be used as a feature table.\n",
    "1. Add new features to an existing feature table in Unity Catalog.\n",
    "1. Explore a Feature Store table in the user interface.\n",
    "1. Upgrade a workspace feature table to a Unity Catalog feature table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad8b836-4916-45e0-979b-f1a1b7cf0bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "   \n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6640f0b7-f0e5-4b8f-a000-3e6c3da2027a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2cdfb9f-ca71-49cc-a37c-2d0856bec10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4beea470-dae1-44bc-9ce4-463e9582c451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed012923-c463-4c61-86f0-578dd4bf7624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01c52e43-2ff1-4929-80e3-3490a60effab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d2d477d-fee8-4923-ac9a-c22adb1c25c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Before we save features to a feature table we need to create features that we are interested in. Feature selection criteria depend on your project goals and business problem. Thus, in this section, we will pick some features, however, it doesn't necessarily mean that these features are significant for our purpose.\n",
    "\n",
    "**One important point is that you need to exclude the target field from the feature table and you need to define a primary key for the table.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b69a34ff-6f29-4e54-887a-4a603c1f89e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Dataset\n",
    "\n",
    "Typically, first, you will need to conduct data pre-processing and select features. As we covered data pre-processing and feature preparation, we will load a clean dataset which you would typically load from a **`silver`** table.\n",
    "\n",
    "Let's load in our dataset from a CSV file containing Telco customer churn data from the specified path using Apache Spark. **In this dataset the target column will be `Churn` and primary key will be `customerID`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb39532-52ba-418b-b36b-cf963653cda5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset with spark\n",
    "shared_volume_name = 'telco' # From Marketplace\n",
    "csv_name = 'telco-customer-churn' # CSV file name\n",
    "dataset_path = f\"{DA.paths.datasets.telco}/{shared_volume_name}/{csv_name}.csv\" # Full path\n",
    "telco_df = spark.read.csv(dataset_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "# # Drop the target column\n",
    "telco_df = telco_df.drop(\"Churn\")\n",
    "\n",
    "# # View dataset\n",
    "display(telco_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cad448f-8313-4489-a156-d3e405075c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save Features to Feature Table\n",
    "\n",
    "\n",
    "Let's start creating a <a href=\"https://docs.databricks.com/en/machine-learning/feature-store/uc/feature-tables-uc.html#install-feature-engineering-in-unity-catalog-python-client\" target=\"_blank\">Feature Engineering Client</a> so we can populate our feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcf2c8e5-8b99-4e2d-848f-426ce70e14d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "help(fe.create_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "041e6ebb-63f2-4488-8a40-a512cc1e05bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Create Feature Table\n",
    "\n",
    "Next, we can create the Feature Table using the **`create_table`** method.\n",
    "\n",
    "This method takes a few parameters as inputs:\n",
    "* **`name`** - A feature table name of the form **`<catalog>.<schema_name>.<table_name>`**\n",
    "* **`primary_keys`** - The primary key(s). If multiple columns are required, specify a list of column names.\n",
    "* **`timestamp_col`** - [OPTIONAL] any timestamp column which can be used for `point-in-time` lookup.\n",
    "* **`df`** - Data to insert into this feature table.  The schema of **`features_df`** will be used as the feature table schema.\n",
    "* **`schema`** - Feature table schema. Note that either **`schema`** or **`features_df`** must be provided.\n",
    "* **`description`** - Description of the feature table\n",
    "* **`partition_columns`** - Column(s) used to partition the feature table.\n",
    "* **`tags`** - Tag(s) to tag feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5437d522-fbce-4441-8d16-63e566758c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # create a feature table from the dataset\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.telco_customer_features\"\n",
    "\n",
    "fe.create_table(\n",
    "    name=table_name,\n",
    "    primary_keys=[\"customerID\"],\n",
    "    df=telco_df,\n",
    "    #partition_columns=[\"InternetService\"] for small datasets partitioning is not recommended\n",
    "    description=\"Telco customer features\",\n",
    "    tags={\"source\": \"bronze\", \"format\": \"delta\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10e9c1e8-9ea6-49a7-bb86-31d494802058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Alternatively, you can **`create_table`** with schema only (without **`df`**), and populate data to the feature table with **`fe.write_table`**, **`fe.write_table`** has **`merge`** mode ONLY (to overwrite, we should drop and then re-create the table).\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "# One time creation\n",
    "fs.create_table(\n",
    "    name=table_name,\n",
    "    primary_keys=[\"index\"],\n",
    "    schema=telco_df.schema,\n",
    "    description=\"Original Telco data (Silver)\"\n",
    ")\n",
    "\n",
    "# Repeated/Scheduled writes\n",
    "fs.write_table(\n",
    "    name=table_name,\n",
    "    df=telco_df,\n",
    "    mode=\"merge\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acdf3829-f399-4e90-818b-ee50699fc526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explore Feature Table with the UI\n",
    "\n",
    "Now let's explore the UI and see how it tracks the tables that we created.\n",
    "\n",
    "* Click on **Features** from left panel.\n",
    "\n",
    "* Select the **catalog** that you used for creating the feature table.\n",
    "\n",
    "* Click on the feature table and you should see the table details as shown below.\n",
    "\n",
    "![ml-01-feature-store-feature-table-v1](../Includes/images/ml-01-feature-store-feature-table-v1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d7c153-9357-4bee-9147-96d98a8785d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Load Feature Table\n",
    "\n",
    "We can also look at the metadata of the feature store via the FeatureStore client by using **`get_table()`**. *As feature table is a Delta table we can load it with Spark as normally we do for other tables*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c4a3895-ca74-4ff7-93f2-b135887303c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ft = fe.get_table(name=table_name)\n",
    "print(f\"Feature Table description: {ft.description}\")\n",
    "print(ft.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bfce53d-59a8-494c-84b5-05db4c7c049a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fe.read_table(name=table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e725db91-c916-424b-865f-87ea0eaf94df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Update Feature Table\n",
    "\n",
    "In some cases we might need to update an existing feature table by adding new features or deleting existing features. In this section, we will show to make these type of changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "151078ce-f8d9-4c60-88ad-0688ab76bc83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add a New Feature\n",
    "\n",
    "To illustrate adding a new feature, let's redefine an existing one. In this case, we'll transform the `tenure` column by categorizing it into three groups: `short`, `mid`, and `long`, representing different tenure durations. \n",
    "\n",
    "Then we will write the dataset back to the feature table. The important parameter is the `mode` parameter, which we should set to `\"merge\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e15d5ec-84dc-4f66-ae7e-499d6925e86d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "telco_df_updated = telco_df.withColumn(\"tenure_group\", \n",
    "    when((telco_df.tenure >= 0) & (telco_df.tenure <= 25), \"short\")\n",
    "    .when((telco_df.tenure > 25) & (telco_df.tenure <= 50), \"mid\")\n",
    "    .when((telco_df.tenure > 50) & (telco_df.tenure <= 75), \"long\")\n",
    "    .otherwise(\"invalid\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37ae25cc-2562-4d51-8a60-215329cf95f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Selecting relevant columns. Use an appropriate mode (e.g., \"merge\") and display the written table for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a3fb343-fab4-470f-affd-8e0425fdb589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe.write_table(\n",
    "    name=table_name,\n",
    "    df=telco_df_updated.select(\"customerID\",\"tenure_group\"), # primary_key and column to add\n",
    "    mode=\"merge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "171afd67-0b09-4d38-b7ac-fbacba9cf60b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Delete Existing Feature\n",
    "\n",
    "To remove a feature column from the table you can just drop the column. Let's drop the original `tenure` column.\n",
    "\n",
    "**\uD83D\uDCA1 Note:** We need to set Delta read and write protocol version manually to support column mapping. If you want to learn more about this you can check related [documentation page](https://docs.databricks.com/en/delta/delta-column-mapping.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d5401a2-907d-4758-99d9-7b6f814ae89d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE telco_customer_features SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name', 'delta.minReaderVersion' = '2', 'delta.minWriterVersion' = '5');\n",
    "ALTER TABLE telco_customer_features DROP COLUMNS (tenure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b486a931-2ef8-4dde-bd2e-843ba7f431da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Feature Table by Version\n",
    "\n",
    "As feature tables are based on Delta tables, we get all nice features of Delta including versioning. To demonstrate this, let's read from a snapshot of the feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aff26cae-8440-476f-a4e5-1ce2e3b6a1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Get timestamp for initial feature table\n",
    "timestamp_v3 = spark.sql(f\"DESCRIBE HISTORY {table_name}\").orderBy(\"version\").collect()[2].timestamp\n",
    "print(timestamp_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5cfdcc-be60-4799-b8c7-25c9f174d4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Read previous version using native spark API\n",
    "telco_df_v3 = (spark\n",
    "        .read\n",
    "        .option(\"timestampAsOf\", timestamp_v3)\n",
    "        .table(table_name))\n",
    "\n",
    "display(telco_df_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3a49c24-83fa-4ee2-b658-22dfab57b6c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Display old version of feature table\n",
    "feature_df = fe.read_table(\n",
    "  name=table_name,\n",
    "  as_of_delta_timestamp=timestamp_v3\n",
    ")\n",
    "\n",
    "feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7f10580-e887-4a44-8dca-68a8a8f12303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Create a Feature Table from Existing UC Table\n",
    "\n",
    "Alter/Change existing UC table to become a feature table\n",
    "Add a primary key (PK) with non-null constraint _(with timestamp if applicable)_ on any UC table to turn it into a feature table (more info [here](https://docs.databricks.com/en/machine-learning/feature-store/uc/feature-tables-uc.html#use-existing-uc-table))\n",
    "\n",
    "In this example, we have a table created in the beginning of the demo which contains security features. Let's convert this delta table to a feature table.\n",
    "\n",
    "For this, we need to do these two changes;\n",
    "\n",
    "1. Set primary key columns to `NOT NULL`.\n",
    "\n",
    "1. Alter the table to add the `Primary Key` Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "764c1e12-e1e1-404e-8cc3-1fea8cf9212c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * FROM security_features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6b89e8c-0b48-4987-935b-9ff801a4c270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE security_features ALTER COLUMN customerID SET NOT NULL;\n",
    "ALTER TABLE security_features ADD CONSTRAINT security_features_pk_constraint PRIMARY KEY(customerID);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a70f11e-e72f-4d70-8614-d4f04c41c634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## _[OPTIONAL]_ Migrate Workspace Feature Table to Unity Catalog\n",
    "\n",
    "If you have a classic/workspace feature table, you can migrate it to Unity Catalog feature store. To do that, first, you will need to upgrade the table to UC supported table and then use `UpgradeClient` to complete the upgrade. For instructions please visit [this documentation page](https://docs.databricks.com/en/machine-learning/feature-store/uc/feature-tables-uc.html#upgrade-a-workspace-feature-table-to-unity-catalog).\n",
    "\n",
    "A sample code snippet for upgrading classic workspace table;\n",
    "\n",
    "```\n",
    "from databricks.feature_engineering import UpgradeClient\n",
    "\n",
    "\n",
    "upgrade_client = UpgradeClient()\n",
    "\n",
    "upgrade_client.upgrade_workspace_table(\n",
    "  source_workspace_table=\"database.test_features_table\",\n",
    "  target_uc_table=f\"{CATALOG}.{SCHEMA}.test_features_table\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43501009-6e8e-4fc5-add5-7c1a63885720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this demo, we learned about Feature Stores, essential for optimizing machine learning models. We explored their benefits, compared Workspace and Unity Catalog Feature Stores, and created feature store tables for effective feature engineering. Mastering these skills empowers efficient collaboration and enhances data consistency, contributing to the development of robust machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aa75934-482e-41a8-a9f8-f96433ecee0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "3.1 Demo - Using Feature Store for Feature Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}